{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 必要なライブラリのインポート\n",
        "import os\n",
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.formula.api as smf\n",
        "import statsmodels.api as sm\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# 日本語フォント設定\n",
        "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "try:\n",
        "    if os.name == 'posix':\n",
        "        japanese_fonts = ['Hiragino Sans', 'Hiragino Kaku Gothic Pro', 'Yu Gothic', 'Meiryo']\n",
        "        for font in japanese_fonts:\n",
        "            try:\n",
        "                plt.rcParams['font.family'] = font\n",
        "                break\n",
        "            except:\n",
        "                continue\n",
        "except:\n",
        "    pass\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "print(\"ライブラリのインポートが完了しました\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_directory_name(dir_name):\n",
        "    params = dir_name.split(',')\n",
        "    if len(params) < 30: return None, None, None\n",
        "    try:\n",
        "        cpnum = next((int(params[i]) for i in range(10) if int(params[i]) != 0), None)\n",
        "        cpnum_range = next((int(params[i]) for i in range(10, 20) if int(params[i]) != 0), None)\n",
        "        cpnum_dir = next((int(params[i]) for i in range(20, 30) if int(params[i]) != 0), None)\n",
        "        return cpnum, cpnum_range, cpnum_dir\n",
        "    except (ValueError, IndexError): return None, None, None\n",
        "\n",
        "def collect_data_task2(logs_root='../../Logs/'):\n",
        "    data_records = []\n",
        "    logs_path = Path(logs_root)\n",
        "    for tree_dir in logs_path.glob('tree=*'):\n",
        "        tree_value = int(tree_dir.name.split('=')[1])\n",
        "        for param_dir in tree_dir.iterdir():\n",
        "            if not param_dir.is_dir(): continue\n",
        "            cpnum, cpnum_range, cpnum_dir = parse_directory_name(param_dir.name)\n",
        "            if cpnum is None: continue\n",
        "            bug_path = param_dir / 'detected_bugs.csv'\n",
        "            if not bug_path.exists(): continue\n",
        "            with open(bug_path, 'r') as f:\n",
        "                bug_results = [row[0] for row in csv.reader(f)]\n",
        "                if len(bug_results) >= 5:\n",
        "                    bug_detected_all = 1 if all(r not in ['timeout', 'null'] for r in bug_results[:5]) else 0\n",
        "                    data_records.append({\n",
        "                        'tree': tree_value, 'cpNum': cpnum, \n",
        "                        'cpNum_range': cpnum_range, 'cpNum_dir': cpnum_dir,\n",
        "                        'bug_detected_all': bug_detected_all\n",
        "                    })\n",
        "    df = pd.DataFrame(data_records)\n",
        "    print(f\"データ収集完了: {len(df)}件のレコード\")\n",
        "    return df\n",
        "\n",
        "df_agg = collect_data_task2()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# タスク2: 5回実行して5回ともバグ発見の予測（GridSearchCVによる最適化）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = df_agg[['tree', 'cpNum', 'cpNum_range', 'cpNum_dir']]\n",
        "y = df_agg['bug_detected_all']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "print(f\"訓練データ: {len(X_train)}, テストデータ: {len(X_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GridSearchCVによる探索"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('lr', LogisticRegression(random_state=42, max_iter=1000, solver='liblinear'))\n",
        "])\n",
        "\n",
        "param_grid = {\n",
        "    'lr__C': np.logspace(-3, 3, 7),\n",
        "    'lr__penalty': ['l1', 'l2'],\n",
        "    'lr__class_weight': [None, 'balanced']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    pipeline, \n",
        "    param_grid, \n",
        "    cv=cv, \n",
        "    scoring='f1', \n",
        "    n_jobs=-1, \n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"探索を開始します...\")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(f\"\\nBest Parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best CV F1-Score: {grid_search.best_score_:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 最良モデルの評価"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "print(\"【テストデータでの評価】\")\n",
        "print(f\"Accuracy:  {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Recall:    {recall_score(y_test, y_pred):.4f}\")\n",
        "print(f\"F1-Score:  {f1_score(y_test, y_pred):.4f}\")\n",
        "\n",
        "print(\"\\n【混同行列】\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "print(\"\\n【モデル係数（全データ再学習後）】\")\n",
        "best_model.fit(X, y)\n",
        "final_lr = best_model.named_steps['lr']\n",
        "for name, coef in zip(X.columns, final_lr.coef_[0]):\n",
        "    print(f\"{name:<12}: {coef:.6f}\")\n",
        "print(f\"{'intercept':<12}: {final_lr.intercept_[0]:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## パラメータ感度分析"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_df = pd.DataFrame(grid_search.cv_results_)\n",
        "results_df['param_lr__class_weight'] = results_df['param_lr__class_weight'].fillna('None')\n",
        "\n",
        "weights = results_df['param_lr__class_weight'].unique()\n",
        "fig, axes = plt.subplots(1, len(weights), figsize=(15, 6), sharey=True)\n",
        "\n",
        "if len(weights) == 1: axes = [axes]\n",
        "\n",
        "for i, weight in enumerate(weights):\n",
        "    subset = results_df[results_df['param_lr__class_weight'] == weight]\n",
        "    pvt = subset.pivot_table(index='param_lr__C', columns='param_lr__penalty', values='mean_test_score')\n",
        "    pvt.index = [f'{x:.3g}' for x in pvt.index]\n",
        "    sns.heatmap(pvt, annot=True, cmap='viridis', ax=axes[i], vmin=results_df['mean_test_score'].min(), vmax=results_df['mean_test_score'].max())\n",
        "    axes[i].set_title(f'class_weight: {weight}')\n",
        "    axes[i].set_ylabel('C (Regularization Strength)')\n",
        "    axes[i].set_xlabel('Penalty Type')\n",
        "\n",
        "plt.suptitle('Hyperparameter Sensitivity: C, Penalty and Class Weight vs F1-Score (Task 2: All Bug)')\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
